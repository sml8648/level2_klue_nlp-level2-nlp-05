import torch
from torch.optim.lr_scheduler import OneCycleLR

# https://huggingface.co/transformers/v3.0.2/_modules/transformers/trainer.html
# https://huggingface.co/course/chapter3/4
import transformers
from transformers import DataCollatorWithPadding, EarlyStoppingCallback
from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoConfig, AutoModel
from transformers import AutoModelForSequenceClassification

import data_loaders.data_loader as dataloader
import utils.util as utils
import model.model as model_arch

import mlflow
import mlflow.sklearn
from azureml.core import Workspace

from datetime import datetime
import re
import os
from omegaconf import OmegaConf
from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union
from collections import defaultdict
from pydoc import locate

from ray import tune
import trainer.trainer as CustomTrainer


class MyDataCollatorWithPadding(DataCollatorWithPadding):
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        max_len = 0
        for i in features:
            if len(i['input_ids']) > max_len : max_len = len(i['input_ids'])

        batch = defaultdict(list)
        for item in features:
            for k in item:
                if('label' not in k):
                    padding_len = max_len - item[k].size(0)
                    if(k == 'input_ids'):
                        item[k] = torch.cat((item[k], torch.tensor([self.tokenizer.pad_token_id]*padding_len)), dim=0)
                    else:
                        item[k] = torch.cat((item[k], torch.tensor([0]*padding_len)), dim=0)
                batch[k].append(item[k])
                
        for k in batch:
            batch[k] = torch.stack(batch[k], dim=0)
            batch[k] = batch[k].to(torch.long)
        return batch


def start_mlflow(experiment_name):
    # Enter details of your AzureML workspace
    subscription_id = "0275dc6c-996d-42d1-8263-8f7b4e81f271"
    resource_group = "basburger"
    workspace_name = "basburger"
    ws = Workspace.get(name=workspace_name, subscription_id=subscription_id, resource_group=resource_group)

    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

    # https://learn.microsoft.com/ko-kr/azure/machine-learning/how-to-log-view-metrics?tabs=interactive
    mlflow.set_experiment(experiment_name)
    # Start the run
    mlflow_run = mlflow.start_run()


def train_hps(conf, hp_conf):
    # ì‹¤í–‰ ì‹œê°„ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
    now = datetime.now()
    train_start_time = now.strftime("%d-%H-%M")

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    model_name = conf.model.model_name
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    # use_fast=Falseë¡œ ìˆ˜ì •í•  ê²½ìš° -> RuntimeError ë°œìƒ
    # RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

    if conf.data.tem == 2: #typed entity tokenì— ì“°ì´ëŠ” ìŠ¤í˜ì…œ í† í°
        special_tokens_dict = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>', '<e3>', '</e3>', '<e4>', '</e4>']}
        tokenizer.add_special_tokens(special_tokens_dict)
        
    data_collator = MyDataCollatorWithPadding(tokenizer=tokenizer)

    # ì´í›„ í† í°ì„ ì¶”ê°€í•˜ëŠ” ê²½ìš° ì´ ë¶€ë¶„ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.
    # tokenizer.add_special_tokens()
    # tokenizer.add_tokens()

    # mlflow ì‹¤í—˜ëª…ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    experiment_name = model_name +'_'+ conf.model.model_class_name + "_bs" + str(conf.train.batch_size) + "_ep" + str(conf.train.max_epoch) + "_lr" + str(conf.train.learning_rate)
    start_mlflow(experiment_name)  # ê°„ë‹¨í•œ ì‹¤í–‰ì„ í•˜ëŠ” ê²½ìš° ì£¼ì„ì²˜ë¦¬ë¥¼ í•˜ì‹œë©´ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë©ë‹ˆë‹¤.

    # load dataset
    RE_train_dataset = dataloader.load_dataset(tokenizer, conf.path.train_path,conf)
    RE_dev_dataset = dataloader.load_dataset(tokenizer, conf.path.dev_path,conf)
    RE_test_dataset = dataloader.load_dataset(tokenizer, conf.path.test_path,conf)

    # ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œëŠ” ê²½ìš° ì´ ë¶€ë¶„ì„ ë°”ê¿”ì£¼ì„¸ìš”.
    continue_train=False
    if continue_train:    
        model_config = AutoConfig.from_pretrained(model_name)
        model = model_arch.CustomRBERT(model_config, conf, len(tokenizer))
        checkpoint = torch.load(conf.path.load_model_path)
        model.load_state_dict(checkpoint)
    elif conf.model.model_class_name == 'TAPT' :
        model = AutoModelForSequenceClassification.from_pretrained(
        conf.path.load_pretrained_model_path, num_labels=30
        )
    else:
        model_class = locate(f'model.model.{conf.model.model_class_name}')
        model = model_class(conf, len(tokenizer))

    model.parameters
    model.to(device)
    # ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ì‹  ê²½ìš° ì´ ë¶€ë¶„ì„ ë°”ê¿”ì£¼ì„¸ìš”.
    optimizer = transformers.AdamW(model.parameters(), lr=conf.train.learning_rate)

    # ì´ë“±ë³€ ì‚¼ê°í˜• í˜•íƒœë¡œ lrì´ ì„œì„œíˆ ì¦ê°€í–ˆë‹¤ê°€ ê°ì†Œí•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ì…ë‹ˆë‹¤.
    # ì²«ì‹œì‘ lr: learning_rate/div_factor, ë§ˆì§€ë§‰ lr: ì²«ì‹œì‘ lr/final_div_factor
    # í•™ìŠµê³¼ì • stepìˆ˜ë¥¼ ê³„ì‚°í•´ ìŠ¤ì¼€ì¤„ëŸ¬ì— ì…ë ¥í•´ì¤ë‹ˆë‹¤. -> steps_per_epoch * epochs / 2 ì§€ì  ê¸°ì¤€ìœ¼ë¡œ lrê°€ ìƒìŠ¹í–ˆë‹¤ê°€ ê°ì†Œ
    steps_per_epoch = len(RE_train_dataset) // conf.train.batch_size + 1 if len(RE_train_dataset) % conf.train.batch_size != 0 else len(RE_train_dataset) // conf.train.batch_size
    scheduler = OneCycleLR(
        optimizer,
        max_lr=conf.train.learning_rate,
        steps_per_epoch=steps_per_epoch,
        pct_start=0.3,
        epochs=conf.train.max_epoch,
        anneal_strategy="linear",
        div_factor=1e100,
        final_div_factor=1,
    )

    # https://docs.ray.io/en/latest/tune/api_docs/search_space.html
    def ray_hp_space(trial):
        return {
            "learning_rate": tune.loguniform(hp_conf.learning_rate.min,hp_conf.learning_rate.max),
            "per_device_train_batch_size": tune.choice([64, 128]), # ì—¬ê¸°ì„œ batch sizeê°€ ë°”ë€Œë¯€ë¡œ ëŒì•„ê°€ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì¸¡ì¤‘!
        }

    # ë¦¬íŒ©í† ë§ í•„ìš”...
    def model_init(trial):
        return model_arch.Model(conf, len(tokenizer))

    training_args = TrainingArguments(
        # output directory ê²½ë¡œ step_saved_model/ì‹¤í–‰ëª¨ë¸/ì‹¤í–‰ì‹œê°(ì¼-ì‹œ-ë¶„)
        # -> ex. step_saved_model/klue-roberta-latge/18-12-04(í‘œì¤€ì‹œê°ì´ë¼ 9ì‹œê°„ ëŠë¦¼)
        # ëª¨ë¸ì´ ê°™ë”ë¼ë„ ì‹¤í–‰í•œ ì‹œê°„ì— ë”°ë¼ ì €ì¥ë˜ëŠ” ê²½ë¡œê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì„œë²„ ìš©ëŸ‰ ê´€ë¦¬ë¥¼ ì˜í•´ì£¼ì„¸ìš”.
        # step_saved_model í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.
        output_dir=f"./step_saved_model/{re.sub('/', '-', model_name)}/{train_start_time}",
        save_total_limit=conf.utils.top_k,  # save_stepsì—ì„œ ì €ì¥í•  ëª¨ë¸ì˜ ìµœëŒ€ ê°œìˆ˜
        save_steps=conf.train.save_steps,  # ì´ stepë§ˆë‹¤ eval_stepsì—ì„œ ê³„ì‚°í•œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
        num_train_epochs=conf.train.max_epoch,  # í•™ìŠµ ì—í¬í¬ ìˆ˜
        learning_rate=conf.train.learning_rate,  # learning_rate
        per_device_train_batch_size=conf.train.batch_size,  # train batch size
        per_device_eval_batch_size=conf.train.batch_size,  # valid batch size
        # weight_decay=0.01,               # strength of weight decay ì´ê±° ë¨¸í•˜ëŠ” ê±´ì§€ ëª¨ë¥´ê² ì–´ìš”.
        logging_dir="./logs",  # directory for storing logs ë¡œê·¸ ê²½ë¡œ ì„¤ì •ì¸ë° í´ë”ê°€ ì•ˆìƒê¹€?
        logging_steps=conf.train.logging_steps,  # í•´ë‹¹ ìŠ¤íƒ­ë§ˆë‹¤ loss, lr, epochê°€ cmdì— ì¶œë ¥ë©ë‹ˆë‹¤.
        evaluation_strategy="steps",
        # `no`: No evaluation during training.
        # `steps`: Evaluate every `eval_steps`.
        # `epoch`: Evaluate every end of epoch.
        eval_steps=conf.train.eval_steps,  # í•´ë‹¹ ìŠ¤íƒ­ë§ˆë‹¤ valid setì„ ì´ìš©í•´ì„œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ save_steps ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.
        load_best_model_at_end=True,
        metric_for_best_model=conf.utils.monitor,  # í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ í•  lossê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    )

     # for hyper parameter search
    custom_trainer = CustomTrainer.CustomTrainer(
        args=training_args,  # training arguments, defined above
        train_dataset=RE_train_dataset,  # training dataset
        eval_dataset=RE_dev_dataset,  # evaluation dataset
        compute_metrics=utils.compute_metrics,  # define metrics function
        data_collator=data_collator,
        model_init=model_init,
        # optimizers=(optimizer, scheduler), # Error : `model_init` is incompatible with `optimizers`
        callbacks=[EarlyStoppingCallback(early_stopping_patience=conf.utils.patience)],
        model=model,  # ğŸ¤— for Transformers model parameter
        conf=conf,
    )

    # https://github.com/huggingface/transformers/blob/84c9cc6d1599e1a64ee73e14ce33727ec865baef/src/transformers/trainer.py
    best_run = custom_trainer.hyperparameter_search(
        direction="maximize",
        backend="ray",
        hp_space=ray_hp_space,  # ì„¤ì •í•œ hyperparameter ë¶ˆëŸ¬ì˜¤ê¸°
        n_trials=2,             # n_traials : search íšŸìˆ˜
        # compute_objective=compute_objective, # defaultê°€ f1-scoreë¼ê³  í•˜ì—¬ ë”°ë¡œ ì„¤ì • X 
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,  # ìœ„ì—ì„œ ì„¤ì •í•œ training_argsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        train_dataset=RE_train_dataset,  # training dataset
        eval_dataset=RE_dev_dataset,  # evaluation dataset
        compute_metrics=utils.compute_metrics,  # utilsì— ìˆëŠ” í‰ê°€ ë§¤íŠ¸ë¦­ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        data_collator=data_collator,
        optimizers=(optimizer, scheduler),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=conf.utils.patience)],
    )

    print("Before:", trainer.args)
    # best_runìœ¼ë¡œ ë°›ì•„ì˜¨ best hyperparameterë¡œ ì¬í•™ìŠµ
    # https://github.com/huggingface/setfit/blob/ebee18ceaecb4414482e0a6b92c97f3f99309d56/scripts/transformers/run_fewshot.py
    for key, value in best_run.hyperparameters.items():
        setattr(trainer.args, key, value)

    print("After:", trainer.args)
    

    trainer.train()
    # train ê³¼ì •ì—ì„œ ê°€ì¥ í‰ê°€ ì ìˆ˜ê°€ ì¢‹ì€ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
    # best_model í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.
    trainer.save_model(f"./best_model/{re.sub('/', '-', model_name)}/{train_start_time}")
    
    # mlflow.end_run()  # ê°„ë‹¨í•œ ì‹¤í–‰ì„ í•˜ëŠ” ê²½ìš° ì£¼ì„ì²˜ë¦¬ë¥¼ í•˜ì‹œë©´ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    model.eval()
    metrics = trainer.evaluate(RE_test_dataset)
    print("Training is complete!")
    print("==================== Test metric score ====================")
    print("eval loss: ", metrics["eval_loss"])
    print("eval auprc: ", metrics["eval_auprc"])
    print("eval micro f1 score: ", metrics["eval_micro f1 score"])
    
    # best_model ì €ì¥í•  ë•Œ ì‚¬ìš©í–ˆë˜ configíŒŒì¼ë„ ê°™ì´ ì €ì¥í•©ë‹ˆë‹¤.
    if not os.path.exists(f"./best_model/{re.sub('/', '-', model_name)}/{train_start_time}"):
        os.makedirs(f"./best_model/{re.sub('/', '-', model_name)}/{train_start_time}")
    with open(f"./best_model/{re.sub('/', '-', model_name)}/{train_start_time}/config.yaml", "w+") as fp:
        OmegaConf.save(config=conf, f=fp.name)


import argparse
from omegaconf import OmegaConf

# í—ˆê¹…í˜ì´ìŠ¤ì— ëª¨ë¸ì„ ì €ì¥í•˜ê³  ì‹¶ìœ¼ì‹œë©´ ì‹¤í–‰ ì „ í„°ë¯¸ë„ì—
# huggingface-cli login ì…ë ¥ í›„
# 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 í† í°ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # ì—¬ê¸°ì„œ omegaconfig íŒŒì¼ ì´ë¦„ ì„¤ì •í•˜ê³  ì‹¤í–‰í•´ì£¼ì„¸ìš”.
    parser.add_argument("--config", "-c", type=str, default="base_config")
    parser.add_argument("--hp_config", type=str, default="hp_search")

    args = parser.parse_args()
    conf = OmegaConf.load(f"./config/{args.config}.yaml")
    hp_conf = OmegaConf.load(f"./config/{args.hp_config}.yaml")
    print("ì‹¤í–‰ ì¤‘ì¸ config file: ", args.config)
    print("ì‹¤í–‰ ì¤‘ì¸ hp file: ", args.hp_config)
    # ì‹œë“œ ì„¤ì •ì„ í•´ì•¼ë ê¹Œìš”?
    # SEED = conf.utils.seed
    # random.seed(SEED)
    # np.random.seed(SEED)
    # torch.manual_seed(SEED)
    # torch.cuda.manual_seed_all(SEED)
    # torch.use_deterministic_algorithms(True)

    # í„°ë¯¸ë„ ì‹¤í–‰ ì˜ˆì‹œ : python main.py -mt -> train.py ì‹¤í–‰
    #                python main.py -mi -> inference.py ì‹¤í–‰

    train_hps(conf, hp_conf)
    

    